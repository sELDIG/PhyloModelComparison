---
title: "Analysis"
format: 
  html:
    embed-resources: true
    self-contained-math: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE}
cacheData = F
```

# Calculate tree metrics and align with model processes

```{r}
source('code/analysis_functions.r')

#metricsForManyTrees(list.files("trees/uniform_sampling_experiment"),
#                    treedir = "trees/uniform_sampling_experiment",
#                    fileOut = "derived_tree_data/USE_treeStats.txt")


# Parameter key across simulations; 
# url <- 'https://docs.google.com/spreadsheets/d/1pcUuINauW11cE5OpHVQf_ZuzHzhm2VJkCn7-lSEJXYI/edit#gid=1171496897'
# paramKey <- gsheet2tbl(url)
# write.csv(paramKey, 'trees/uniform_sampling_experiment/paramKeys/simulation_parameters_key.csv', row.names = F)

# Align parameter values with the relevant experiment, create a dataframe with columns for
# model, model2 (with scenario suffix if appropriate), simID, and columns for the parameter
# names and parameter values associated with 5 processes: env, dis, nic, mut, com

modelList = c('ca', 'fh', 'gen', 'hs', 'pontarp', 've', 'xe', 'la')

for (m in modelList) {
  
  if (!exists("processDF")) {
    processDF = alignParametersWithProcesses(m, paramKeyPath = 'trees/uniform_sampling_experiment/paramKeys')
  } else {
    processDF = rbind(processDF, alignParametersWithProcesses(m, paramKeyPath = 'trees/uniform_sampling_experiment/paramKeys'))
  }
  
}

# Join tree metrics to the aligned parameter-process dataframe: for analysis, use **processDFmetrics**
# Script for calculating tree metrics for trees that have not already been analyzed

# Read in existing output file
treeOutput = read.table('derived_tree_data/USE_treeStats.txt', sep = '\t', header = T)

# Join process-parameter linkage dataframe to tree output
allSimuData = left_join(processDF, treeOutput, by = c('model', 'simID'))

write.csv(simuData, 
          'derived_tree_data/process_parameter_values_and_tree_metrics_sign_corrected.csv', row.names = F)


```

# Read and process data

```{r}
library(RColorBrewer)
library(dplyr)
library(tidyr)
library(ranger)
library(stringr)
library(heatmaply)

# Read in simulated tree data
allSimuData <- read.csv("derived_tree_data/process_parameter_values_and_tree_metrics_sign_corrected.csv", stringsAsFactors = T)

simuData = dplyr::select(allSimuData, -tree_height, -crown_age)

# Visualize correlation matrix of tree metrics
metricCors = cor(simuData[, 24:75], use = "pairwise.complete.obs")

# Metrics strongly correlated (> 0.9) with number_of_lineages
richnessCorrelatedMetrics = row.names(metricCors)[metricCors[,1] >= 0.9]

# all 52 tree metrics
statisticsIndices1 = which(names(simuData) %in% row.names(metricCors))
# Only the tree metrics not strongly correlated with richness
statisticsIndices2 = which(names(simuData) %in% row.names(metricCors) & 
                             !names(simuData) %in% richnessCorrelatedMetrics) 

# Indices for the 5 simulation processes (env, dis, mut, nic, com)
predictorsIndices = c(14,16,18,20,22)
predictors = colnames(simuData)[c(14,16,18,20,22)]
numModelsPerPredictor = c(5, 7, 4, 7, 6)

# 8 models (accounting for abbreviation changes post-experiment)
modelAbbrevs = data.frame(original = c('ca', 'fh', 'gen', 'hs', 'la', 'pontarp', 've', 'xe'), new = c('ca', 'fh', 'ge', 'hs', 'am', 'pw', 've', 'xe'))

models = modelAbbrevs$original


```

# Clustering / redundancy analysis

TODO - add correlation analysis between metrics that was previously in the method section

```{r}
# Correlation among indices
corrs = cor(simuData[, statisticsIndices1], use = "pairwise.complete.obs", )

metricNames = colnames(simuData)[statisticsIndices1]

colnames(corrs) = metricNames
rownames(corrs) = metricNames

par(mar = c(8, 8, 10, 8))
heatmaply(corrs, colors = cool_warm)

```

# Consistency analysis

## Calculating correlation coefficients

Calculating individual as well as model-averaged correlation coefficients between all metrics and processes

```{r, cache=cacheData}
# NOTE: Change next line to statisticsIndices2 to view the smaller subset of metrics not strongly correlated with richness
statisticsIndices = statisticsIndices1
statistics = colnames(simuData)[statisticsIndices]
nStats = length(statisticsIndices)

R2Mat = matrix(nrow = length(statistics), 
                 ncol = length(predictors),
                 dimnames = list(statistics, predictors))

numAgreements = numSigAgreements = meanCorr = medianCorr = R2Mat

R2MatFull = pMatFull = array(
    dim = c(length(statistics), length(predictors), length(models)),
    dimnames = list(statistics, predictors, models)) 

alpha = 0.01 # threshold for counting "significant" correlations

for(i in 1:length(statisticsIndices)){
  for(j in 1:length(predictorsIndices)){
    R2 = pvalues = sign = rep(NA, length(models))
    for(k in 1:length(models)){
      tmp = simuData[simuData$model == models[k],]
      x = scale(tmp[,statisticsIndices[i]])
      y = tmp[,predictorsIndices[j]]
      if(! all(is.na(y))){
      
        corRes = cor.test(x,y, method = "kendall")
        
        R2[k] = corRes$estimate
        pvalues[k] = corRes$p.value
        sign[k] = sign(corRes$estimate)

      } else{
        R2[k] = NA
        pvalues[k] = NA
        sign[k] = NA
      }
    }
    R2MatFull[i,j,] = R2
    pMatFull[i,j,] = pvalues
    numAgreements[i,j] = max(sum(sign == 1, na.rm = T), sum(sign == -1, na.rm = T))
    numSigAgreements[i,j] = max(sum(sign[pvalues <= alpha] == 1, na.rm = T), 
                      sum(sign[pvalues <= alpha] == -1, na.rm = T))
    meanCorr[i,j] = mean(R2, na.rm = T)
    medianCorr[i,j] = median(R2, na.rm = T)
  }
}

```

## Full plot

Definition of image plot

```{r}
image.real3d <- function(mat, mat2, xCol = c("darkblue","blue", "lightblue", "gray94", "pink","red", "darkred"), 
                       range = c(-1,1), x.labels = dimnames(mat)[[1]],      y.labels = dimnames(mat)[[2]], alpha = .01, cex.metrics = 0.5, 
                       cex.models = 0.7, cex.processes = 1) { 
  
  newMat = matrix(nrow = dim(mat)[1], ncol = 50)
  for(i in 1:dim(mat)[1]){
    for(k in 1:dim(mat)[2])
      newMat[i,(1+(k-1)*10):((k-1)*10 + dim(mat)[3])] = mat[i,k,]
  }
  
  newMat <- t(newMat)
  xpos = rep(0:(dim(mat)[2]-1), each = dim(mat)[3])*10 + 1:dim(mat)[3] 
  ypos = 1:dim(mat)[1]
  fields::image.plot(x = 1:50, y = 1:dim(mat)[1], z = newMat, axes = FALSE, zlim = range, 
                     col = colorRampPalette(xCol)(30), xlab = "", ylab = "")
  
  abline(v = c(0,10,20,30,40,50) + 0.5)
  abline(v = c(0,10,20,30,40,50) - 1.5)
  abline(h = c(1:(dim(mat)[1] +1)) + 0.5)
  axis(2, at = 1:nrow(mat), labels = x.labels, las = 2, cex.axis = cex.metrics)
  axis(3, at = seq(5, 45, by = 10), labels = y.labels, cex.axis = cex.processes, las = 1)
  axis(1, at = xpos, labels = rep(dimnames(mat)[[3]], 5), cex.axis = cex.models, las = 2)
  
  # text(x = xpos, y = 0, srt = 45, adj = 1, xpd = TRUE, labels = rep(dimnames(mat)[[3]], 5), cex = 0.65)
  box() 
  
  for(i in 1:dim(mat2)[1]){
    for(k in 1:dim(mat2)[2]){
       for(j in 1:dim(mat2)[3])
        if(!is.na(mat2[i,k,j]) & mat2[i,k,j] < alpha){
          text(x = 10*(k-1) +j,  y = i, labels = "*" )
      }
    }
  }
}
```

```{r}
# Correlation plot with ALL tree metrics for supplement

par(mfrow = c(1,1), mar = c(5,15,5,3))

# Tidier labels
alt.x.labels = dimnames(R2MatFull)[[1]]

# Modifying a few model abbreviations here (e.g. am and pw)
dimnames(R2MatFull)[[3]] = c("ca", "fh", "ge", "hs", "am", "pw", "ve", "xe")

pdf('figures/all_metric_correlations.pdf', height = 10.5, width = 13)
image.real3d(R2MatFull, pMatFull, x.labels = alt.x.labels, y.labels = c("env. filtering", "dispersal", "niche conserv.", "mut./speciation", "competition"),
             cex.metrics = 0.8, cex.models = .9)

mtext("Models", 1, line = 3.5, cex = 2)
mtext("Metrics", 2, at = nrow(R2MatFull)/2, cex = 2, line = 8)
mtext("Processes", 3, cex = 2, line = 3)
dev.off()

# Correlation plot highlighting metrics with strong correlation agreement

agreement = numSigAgreements[(numAgreements[,1] >= 5 & numSigAgreements[,1] >= 4) |
                            (numAgreements[,2] >= 7 & numSigAgreements[,2] >= 6) | 
                            (numAgreements[,3] >= 4 & numSigAgreements[,3] >= 3) | 
                            (numAgreements[,4] >=7 & numSigAgreements[,4] >=6) | 
                            (numAgreements[,5] >= 6 & numSigAgreements[,5] >= 5),]

env.metrics = rownames(agreement)[agreement[,1] >= 4]
dis.metrics = rownames(agreement)[agreement[,2] >= 6]
nic.metrics = rownames(agreement)[agreement[,3] >= 3]
mut.metrics = rownames(agreement)[agreement[,4] >= 6]
com.metrics = rownames(agreement)[agreement[,5] >= 5]

highlightMat = R2MatFull[dimnames(R2MatFull)[[1]] %in% rownames(agreement),,]
pMatHighlight = pMatFull[dimnames(R2MatFull)[[1]] %in% rownames(agreement),,]

par(mar = c(5,10,5,3))

# Tidier labels
alt.x.labels = dimnames(highlightMat)[[1]]

# Modifying a few model abbreviations here (e.g. am and pw)
dimnames(highlightMat)[[3]] = c("ca", "fh", "ge", "hs", "am", "pw", "ve", "xe")

image.real3d(highlightMat, pMatHighlight, x.labels = alt.x.labels, 
             y.labels = c("env. filtering", "dispersal", "niche conserv.", "mut./speciation", "competition"),
             cex.metrics = .8, cex.models = .8)

mtext("Models", 1, line = 3.5, cex = 2)
mtext("Metrics", 2, at = nrow(highlightMat)/2, cex = 2, line = 8)
mtext("Processes", 3, cex = 2, line = 3)

```

# Model inversion

```{r}
library(ranger)
```

## Calculating RF and CV RF values

To be fair, we run this analysis using only the tree metrics that are not strongly correlated with richness

```{r, cache=cacheData}

# Tree statistics not strongly correlated with richness
statistics2 = colnames(simuData)[statisticsIndices2]

predictability2 = array(dim = c(length(statistics2), 
                                length(predictors), 
                                length(models)))

predictability2R2 = array(dim = c(length(predictors), 
                                  length(models)))
rownames(predictability2R2) = predictors
colnames(predictability2R2) = models
predictabilityR2CV = predictability2R2

for(j in 1:length(predictorsIndices)){
  for(k in 1:length(models)){
    tmp = simuData[simuData$model == models[k],c(predictorsIndices[j], statisticsIndices2)]
    tmp = tmp[complete.cases(tmp),]
    if(nrow(tmp) > 0){
      form = as.formula(paste(predictors[j], "~ ."))
      x = ranger(form, data = tmp, 
                 importance = 'permutation',
                scale.permutation.importance = T)
      predictability2[,j,k] = importance(x)     
      predictability2R2[j,k] = x$r.squared
      
      cvindices = sample(1:5, size = nrow(tmp), replace = T)
      r2 = rep(NA, 5)
      for(i in 1:5){
         x = ranger(form, data = tmp[cvindices != i, ])
         xp = predict(x, data = tmp[cvindices == i,] )
         r2[i] = cor(tmp[cvindices == i, 1], xp$predictions)
      }
      predictabilityR2CV[j,k] = mean(r2)
    } 
  }
}
```

## Predictability within a model

The internally calculated R2 values of a RF can sometimes be misleading. That's why we additionally calculated R2 values from a 5-fold CV in the calculation loop above. Results are very similar.

```{r}
modelColors = data.frame(model = models,
                         newLabel = c('ca', 'fh', 'ge', 'hs', 'am', 'pw', 've', 'xe'),
                         color = c(rgb(0,0,0),
                                   rgb(230/255, 159/255, 0),
                                   rgb(86/255, 180/255, 233/255),
                                   rgb(0, 158/255, 115/255),
                                   rgb(240/255, 228/255, 66/255),
                                   rgb(0, 114/255, 178/255),
                                   rgb(213/255, 94/255, 0),
                                   rgb(204/255, 121/255, 167/255)))

par(mar = c(4, 6, 0, 0), mgp = c(4, 1, 0))

# Squaring the correlation values of predictabilityR2CV to get R2's
x = barplot(t(predictabilityR2CV^2), beside = T, las = 1, horiz = F, ylim = c(0,1.1), ylab = expression("Predicted-observed " ~ R^2), xaxt = "n", cex.lab = 1.8, cex.axis = 1.6, col = modelColors$color)

# Add 'x' where process is missing from model
missing.x = x[is.na(t(predictabilityR2CV))]
text(x = missing.x, y = rep(0.02, length(missing.x)), '*', cex = 2)

axis(1, at = c(5, 14, 23, 32, 41), labels = c("env.\nfiltering", "dispersal", "niche\nconserv.", "mutation/\nspeciation", "competition"), cex.axis = 1.6, line = 1.3, tick = F)

axis(1, at = c(0, 9.5, 18.5, 27.5, 36.5, 45.5), tick = T, tck = -0.04, labels = F)

# Legend
text(6, 1.01, "Models", cex = 2)
legend(0, 1, legend = modelColors$newLabel, fill = modelColors$color, ncol = 2, bty = 'n', cex = 1.5)

```

## Between models

As a next step, we calculate predictability between models, i.e. we use the random forests tuned for one model to predict to the next model.

Cross-model predictability is measured by Kendall's rank correlation coefficient.

For the diagonal (within-model predictability), we use the CV predictability calculated in the previous step.

```{r}
# Modified version of image.plot
image.real <- function(mat, xCol = c("blue", "white", "white", "red"), 
                       range = c(-1,1), x.labels = rownames(mat), 
                       y.labels = colnames(mat), cex.axis = 1) { 
  mat <- t(mat)[,nrow(mat):1]
  fields::image.plot(mat, axes = FALSE, zlim = range, 
                     col = colorRampPalette(xCol)(30))
  axis(1, at = seq(0, 1, length = nrow(mat)), labels = x.labels, cex.axis = cex.axis)
  axis(2, at = seq(0, 1, length = ncol(mat)), labels = y.labels, las = 2, cex.axis = cex.axis)
  box() 
}


# Function for plotting cross-model predictability
#   set the processIndex to the process which is being evaluated
#   1 - env, 2 - dis, 3 - nic, 4 - mut, 5 - com

#   create a vector of models that implement the process specified above by
#   excluding those that do not implement it
#   1 - ca, 2 - fh, 3 - ge, 4 - hs, 5 - am, 6 - pw, 7 - ve, 8 - xe

crossPredictability = function(processIndex, modelsToExclude,
                              iterations = 100) {
  
  modelsTMP = models[-modelsToExclude]

  CpredictabilityR2raw = array(dim = c(length(modelsTMP), length(modelsTMP), iterations))

  for (i in 1:iterations) {

    for(k in 1:length(modelsTMP)){
      train = simuData[simuData$model == modelsTMP[k],
                       c(predictorsIndices[processIndex],
                         statisticsIndices2)]
      train = train[complete.cases(train),]
      if(nrow(train) > 0){
        form = as.formula(paste(predictors[processIndex], "~ ."))
        x = ranger(form, data = train)
      } 
      for(j in 1:length(modelsTMP)){
        test = simuData[simuData$model == modelsTMP[j],
                        c(predictorsIndices[processIndex], statisticsIndices2)]
        test = test[complete.cases(test),]
        if(nrow(test) > 0 & k != j){
          y = predict(x, test)
          CpredictabilityR2raw[k,j,i] = cor(y$predictions, test[,1], 
                                         method = "kendall")
        } # end if
      } # end j testing 
    } # end k training
  } # end i iterations

  # Average across all iterations to get a mean correlation
  CpredictabilityR2 = rowMeans(CpredictabilityR2raw, dims = 2)

  rownames(CpredictabilityR2) = modelAbbrevs$new[modelAbbrevs$original %in% modelsTMP]
  colnames(CpredictabilityR2) = modelAbbrevs$new[modelAbbrevs$original %in% modelsTMP]

  # within-model predictability
  tmp = predictabilityR2CV[processIndex,-modelsToExclude]
  for(i in 1:length(modelsTMP)) CpredictabilityR2[i,i] = tmp[i]

  return(CpredictabilityR2)
}

  
crossPredictabilityPlot = function(CpredictabilityR2,
                                   cexValues = 1, cexMain = 2, 
                                   cexAxis = 1.5, cexLab = 1.8,
                                   mainText,
                                   xCol = c("darkblue","blue",
                                            "lightblue", "white",
                                            "pink","red", "darkred"))
{
  
  image.real(CpredictabilityR2, range = c(-1,1), xCol = xCol, cex.axis = cexAxis)
  title(main = mainText, xlab = "Model predicted to", 
        ylab = "Model trained on\n", cex.lab = cexLab, cex.main = cexMain)

  # add values to cells
  for(i in 1:length(modelsTMP)){
    for(j in 1:length(modelsTMP)){
      text((j-1)/(length(modelsTMP)-1),
           1-(i-1)/(length(modelsTMP)-1), 
           labels = format(round(CpredictabilityR2[i,j], digits = 2), nsmall = 2),
           cex = cexValues)
  
    }
  }
}



# 2-panel figure showing cross-predictability for a) dispersal and b) speciation
# This takes several minutes.

set.seed(1)
speciationCP = crossPredictability(processIndex = 4, modelsToExclude = 6, iterations = 100)

dispersalCP = crossPredictability(processIndex = 2, modelsToExclude = 5, iterations = 100)


par(mfrow = c(1, 2), mar = c(5,7,4,4), mgp = c(3, 1, 0), oma = c(0, 0, 0, 1))

crossPredictabilityPlot(speciationCP, cexLab = 2,
                    cexAxis = 1.7, cexValues = 1.3, cexMain = 2,
                    mainText = "Mutation/speciation")
mtext("a)", 2, at= 1.2, las = 1, cex = 2.5, line = 2)

crossPredictabilityPlot(dispersalCP, cexLab = 2,
                    cexAxis = 1.7, cexValues = 1.3, cexMain = 2,
                    mainText = "Dispersal")
mtext("b)", 2, at = 1.2, las = 1, cex = 2.5, line = 2)
```

# Case studies

## within each model

Run random forests within each simulation model in order to predict model parameter values from simulation-derived tree shape metrics. Then plug in empirical tree shape metrics to infer a process parameter value assuming that the tree arose from the processes represented by a particular simulation model.

```{r}

# run a version of metricsForManyTrees() to get tree metric output for empirical sister clades (but that function assumes diff model ID naming convention, so had to manually tweak)
allEmpiricalMetrics = read.table('derived_tree_data/empiricalSisterClades_treeStats.txt', header = T, sep = '\t')
# Filter out hippo and primate clade pairs because of small size
empiricalMetrics = filter(allEmpiricalMetrics, !simID %in% c(3,7))
empiricalMetrics$clade = rep(c(1,2), 6)
empiricalMetrics$pair = empiricalMetrics$simID
empiricalMetrics$cladeName = c('Xenarthra', 'Afrotheria',
                               'Carnivora/\nPerissodactyla', 'Cetartiodactyla',
                               'Apodi', 'Trochili',
                               'Rodentia', 'Lagomorpha',
                               'Coraciiformes', 'Passeriformes',
                               'Pelecaniformes', 'Ciconiiformes')

# The metrics not strongly correlated with richness that were used for RF models
empMetrics2 = empiricalMetrics[, statistics2]

predictedValues2reps = data.frame(tree = character(),
                              rep = numeric(),
                              model = character(), 
                              predictor = character(),
                              value = numeric())

predictability2 = array(dim = c(length(statistics2), length(predictors), length(models)))
predictability2R2 = array(dim = c(length(predictors), length(models)))

# For each combination of process (predictorsIndices) and simulation model, infer the strength of process for each empirical tree based on the RF

# Run over multiple iterations to account for stochasticity in RF
iterations = 100

set.seed(1)
for (i in 1:iterations) {
  for(j in 1:length(predictorsIndices)){
    for(k in 1:length(models)){
      tmp = simuData[simuData$model == models[k],c(predictorsIndices[j],
                                                   statisticsIndices2)]
      tmp = tmp[complete.cases(tmp),]
      if(nrow(tmp) > 0){
        form = as.formula(paste(predictors[j], "~ ."))
        x = ranger(form, data = tmp, importance = 'permutation',
                   scale.permutation.importance = T)
        predictions = predict(x, data = empMetrics2)
      
        tmpPredictions = data.frame(tree = word(empiricalMetrics$tree, 1, sep = fixed('.')),
                                    rep = rep(i, nrow(empMetrics2)),
                                    model = rep(models[k], nrow(empMetrics2)),
                                    predictor = rep(predictors[j], nrow(empMetrics2)),
                                    value = predictions$predictions)
      
        predictedValues2reps = rbind(predictedValues2reps, tmpPredictions)
      
      } # end if
    } # end models
  } # end predictors
} # end iterations

predictedValues2 = predictedValues2reps %>%
  group_by(tree, model, predictor) %>%
  summarize(value = mean(value, na.rm = T)) %>%
  arrange(predictor, model, tree)


# Need to standardize the predicted parameter values relative to the range of values examined within a given model.  
#   scaled = (x - min) / (max - min)          

# So first need to identify min and max values for each model-parameter combo:
# (this generates lots of warnings because of -Inf and Inf values but these can be ignored)
modelParamRange = simuData %>%
  group_by(model) %>%
  summarize(env1Min = min(env1, na.rm = T),
            env1Max = max(env1, na.rm = T),
            com1Min = min(com1, na.rm = T),
            com1Max = max(com1, na.rm = T),
            dis1Min = min(dis1, na.rm = T),
            dis1Max = max(dis1, na.rm = T),
            nic1Min = min(nic1, na.rm = T),
            nic1Max = max(nic1, na.rm = T),
            mut1Min = min(mut1, na.rm = T),
            mut1Max = max(mut1, na.rm = T))

# Note that for some model/parameter combinations, the modeler varied parameter values uniformly on a log scale, while for others values varied uniformly on an arithmetic scale.

# I examined the skewness values of all sets of values, and determined that each of the following combinations had a skewness of 0.7 or greater and merited being log-transformed prior to scaling. 
# (Otherwise, the scaled values, e.g. in model 'hs' for dis1 or mut1 are close to 0)

# ca: mut1, com1
# hs: dis1, mut1
# xe: dis1, mut1

modelParamRange$com1Min[modelParamRange$model == 'ca'] = log10(modelParamRange$com1Min[modelParamRange$model == 'ca'])
modelParamRange$com1Max[modelParamRange$model == 'ca'] = log10(modelParamRange$com1Max[modelParamRange$model == 'ca'])
modelParamRange$dis1Min[modelParamRange$model %in% c('hs', 'xe')] = log10(modelParamRange$dis1Min[modelParamRange$model %in% c('hs', 'xe')])
modelParamRange$dis1Max[modelParamRange$model %in% c('hs', 'xe')] = log10(modelParamRange$dis1Max[modelParamRange$model %in% c('hs', 'xe')])
modelParamRange$mut1Min[modelParamRange$model %in% c('hs', 'xe', 'ca')] = log10(modelParamRange$mut1Min[modelParamRange$model %in% c('hs', 'xe', 'ca')])
modelParamRange$mut1Max[modelParamRange$model %in% c('hs', 'xe', 'ca')] = log10(modelParamRange$mut1Max[modelParamRange$model %in% c('hs', 'xe', 'ca')])

# Rearrange
modelParamMinMax = data.frame(model = rep(modelParamRange$model, 5), 
                              predictor = rep(c('env1', 'com1', 'dis1', 'nic1', 'mut1'), each = 8),
                              minVal = c(modelParamRange$env1Min, modelParamRange$com1Min, modelParamRange$dis1Min, 
                                         modelParamRange$nic1Min, modelParamRange$mut1Min),
                              maxVal = c(modelParamRange$env1Max, modelParamRange$com1Max, modelParamRange$dis1Max, 
                                         modelParamRange$nic1Max, modelParamRange$mut1Max))


# Also need to log-transform the same parameter values in predictedValues2
predictedValues2$value[predictedValues2$model == 'ca' & predictedValues2$predictor %in% c('mut1', 'com1')] = 
  log10(predictedValues2$value[predictedValues2$model == 'ca' & predictedValues2$predictor %in% c('mut1', 'com1')])

predictedValues2$value[predictedValues2$model %in% c('hs', 'xe') & predictedValues2$predictor %in% c('mut1', 'dis1')] = 
  log10(predictedValues2$value[predictedValues2$model %in% c('hs', 'xe') & predictedValues2$predictor %in% c('mut1', 'dis1')])


# Function for rescaling a value between the min and max such that the scaled value varies between 0 and 1.
# If either the min or max are NA, Inf, or -Inf, returns NA
scaleFunction = function(value, min, max) {
  
  if (!min %in% c(-Inf, NA, Inf) & !max %in% c(-Inf, NA, Inf)) {
    
    scaledValue = (value - min) / (max - min)
  
  } else {
  
    scaledValue = NA
  
  }
  return(scaledValue)
}




scaledPredictions2 = predictedValues2 %>%
  left_join(modelParamMinMax, by = c('model', 'predictor')) %>%
  mutate(pair = str_extract(tree, "[1-9]"))

scaledPredictions2$clade = rep(c(1, 2), nrow(predictedvalues2)/2)

scaledPredictions2$scaledValue = apply(scaledPredictions2[, c('value', 'minVal', 'maxVal')], 1, 
                             function(x) scaleFunction(value = x[1], min = x[2], max = x[3]))

scaledPredictions2 = left_join(scaledPredictions2, modelColors, by = 'model')



# Plot of model inversion for dispersal across 6 empirical sister clade pairs
# --bottom panel illustrates relative magnitude of mean.Iprime which was identified as the tree metric most diagnostic of dispersal

# Determines order of panels:
pairs = c(1, 2, 5, 4, 6, 8)

# Specify dispersal process
pro = 'dis1' 

par(mfrow = c(2, 3), mar = c(5, 3, 1, 1), oma = c(0, 3, 0, 0))

panelLetters = letters[1:6]
panelIndex = 0

for (p in pairs) {
  
  panelIndex = panelIndex + 1
    
  tmp = filter(scaledPredictions2, pair == p, predictor == pro)
    
  plot(c(0.5,2.5), range(tmp$scaledValue), type = 'n', xaxt = 'n', xlab = "", ylab = "", las = 1,
       ylim = c(-.3, 1), yaxt = 'n')
  
  mtext(empiricalMetrics$cladeName[empiricalMetrics$pair == p], 1, at = 1:2, line = 2.5, cex = 1.1, padj = 0.5)
  offset = .22
  mtext(c("mean I'", "VRD", "mean I'", "VRD"), 1, at = c(1.1 - offset, 1.05 + offset, 2.05 - offset, 2. + offset), line = .5, cex = .8)
  
  axis(2, at = seq(0.1, 1, by = 0.1), las = 1)
 
  abline(h = 0.1)
     
  for (m in unique(tmp$model)) {
      
    points(tmp$clade[tmp$model==m], tmp$scaledValue[tmp$model==m], type = 'b', col = tmp$color[tmp$model==m], lwd = 3)
    text(2.1, tmp$scaledValue[tmp$model==m & tmp$clade == 2], tmp$newLabel[tmp$model==m], adj = 0)
  }
  
  # panel label
  text(.55, 0.97, paste0("(", panelLetters[panelIndex], ")"), cex = 2)

  # Add barplots showing empirical values of mean.Iprime and VRD for the two sister clades
  par(new = T)
  
  tmpBarData = empiricalMetrics[empiricalMetrics$pair == p, c('i_stat', 'var_depth')] %>%
    rowwise() %>%
    mutate(quantile.i_stat = sum(i_stat > simuData$i_stat, na.rm = T)/nrow(simuData),
           quantile.var_depth = sum(var_depth > simuData$var_depth, na.rm = T)/nrow(simuData)) %>%
    dplyr::select(quantile.i_stat, quantile.var_depth) %>%
    t() %>%
    as.matrix()
  
  
  barplot(tmpBarData, beside = T, yaxt = 'n', ylim = c(0, 3), border = NA, xlim = c(0.5, 7), xaxt = 'n', space = c(0.1, 1.2))
  
  #barplot(as.matrix(tmpBarData[, 3:4]), 
  #        yaxt = 'n', ylim = c(0, 2.5), space = .95, border = NA, xlim = c(0.25, 4.5))  
  
  if (panelIndex %in% c(1, 4)) {
    mtext("Tree Metrics\n(quantiles)", 2, cex = .9, line = 1.8, at = 0.37)
    mtext("Inferred Dispersal", 2, cex = 1.1, line = 3, at = 2)
  }
  
}

###########################
# Plot Inferred Speciation

# Plot of model inversion for dispersal across 6 empirical sister clade pairs
# --bottom panel illustrates relative magnitude of mean.Iprime which was identified as the tree metric most diagnostic of dispersal

# Determines order of panels:
pairs = c(1, 2, 5, 4, 6, 8)

# Specify speciation process
pro = 'mut1' 

par(mfrow = c(2, 3), mar = c(5, 3, 1, 1), oma = c(0, 3, 0, 0))

panelLetters = letters[1:6]
panelIndex = 0

for (p in pairs) {
  
  panelIndex = panelIndex + 1
    
  tmp = filter(scaledPredictions2, pair == p, predictor == pro)
    
  plot(c(0.5,2.5), range(tmp$scaledValue), type = 'n', xaxt = 'n', xlab = "", ylab = "", las = 1,
       ylim = c(-.3, 1), yaxt = 'n')
  
  mtext(empiricalMetrics$cladeName[empiricalMetrics$pair == p], 1, at = 1:2, line = 2.5, cex = 1.1, padj = 0.5)
  offset = .22
  mtext(c("MBL", "LSP", "MBL", "LSP"), 1, at = c(1.1 - offset, 1.05 + offset, 2.05 - offset, 2. + offset), line = .5, cex = .8)
  
  axis(2, at = seq(0.1, 1, by = 0.1), las = 1)
 
  abline(h = 0.1)
     
  for (m in unique(tmp$model)) {
      
    points(tmp$clade[tmp$model==m], tmp$scaledValue[tmp$model==m], type = 'b', col = tmp$color[tmp$model==m], lwd = 3)
    text(2.1, tmp$scaledValue[tmp$model==m & tmp$clade == 2], tmp$newLabel[tmp$model==m], adj = 0)
  }
  
  # panel label
  text(.55, 0.97, paste0("(", panelLetters[panelIndex], ")"), cex = 2)

  # Add barplots showing empirical values of mean_branch_length and laplac_spectrum_p for the two sister clades
  par(new = T)
  
  tmpBarData = empiricalMetrics[empiricalMetrics$pair == p, c('mean_branch_length', 'laplac_spectrum_p')] %>%
    rowwise() %>%
    mutate(quantile.mbl = sum(mean_branch_length > simuData$mean_branch_length, na.rm = T)/nrow(simuData),
           quantile.lsp = sum(laplac_spectrum_p > simuData$laplac_spectrum_p, na.rm = T)/nrow(simuData)) %>%
    dplyr::select(quantile.mbl, quantile.lsp) %>%
    t() %>%
    as.matrix()
  
  
  barplot(tmpBarData, beside = T, yaxt = 'n', ylim = c(0, 3), border = NA, xlim = c(0.5, 7), xaxt = 'n', space = c(0.1, 1.2))
  
  #barplot(as.matrix(tmpBarData[, 3:4]), 
  #        yaxt = 'n', ylim = c(0, 2.5), space = .95, border = NA, xlim = c(0.25, 4.5))  
  
  if (panelIndex %in% c(1, 4)) {
    mtext("Tree Metrics\n(quantiles)", 2, cex = .9, line = 1.8, at = 0.37)
    mtext("Inferred Speciation", 2, cex = 1.1, line = 3, at = 2)
  }
  
}


```

# Supplemental Figures

```{r}

# Switching out 2 model abbreviations
simuData$model3 = simuData$model
simuData$model3 = gsub("pontarp", "pw", simuData$model3)
simuData$model3 = gsub("la", "am", simuData$model3)
simuData$model3 = gsub("gen", "ge", simuData$model3)

par(mfrow = c(7, 5), mar = c(2, 2, 2, 1), mgp = c(2.5, .6, 0))

boxplot(log10(simuData$number_of_lineages) ~ simuData$model3, main = 'log # of lineages', col = rainbow(8), cex.axis = .85, tck = -0.02)

for (i in statisticsIndices2) { 
  boxplot(simuData[,i] ~ simuData$model3, main = names(simuData)[i], col = rainbow(8), cex.axis = .85, tck = -0.02)
}
```

# Reproducibility information

```{r}
sessionInfo()
```
